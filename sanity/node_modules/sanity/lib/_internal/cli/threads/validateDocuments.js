'use strict';

var worker_threads = require('worker_threads');
var readline = require('readline');
var stream = require('stream');
var os = require('os');
var fs = require('fs');
var path = require('path');
var client = require('@sanity/client');
var types = require('@sanity/types');
var getStudioConfig = require('../../../_chunks/getStudioConfig-JSkc4GE0.js');
var mockBrowserEnvironment = require('../../../_chunks/mockBrowserEnvironment-1iCrZBr6.js');
var workerChannels = require('../../../_chunks/workerChannels-ZUUNsU1E.js');
var sanity = require('sanity');
function _interopDefaultCompat(e) {
  return e && typeof e === 'object' && 'default' in e ? e : {
    default: e
  };
}
var readline__default = /*#__PURE__*/_interopDefaultCompat(readline);
var os__default = /*#__PURE__*/_interopDefaultCompat(os);
var fs__default = /*#__PURE__*/_interopDefaultCompat(fs);
var path__default = /*#__PURE__*/_interopDefaultCompat(path);
const MAX_VALIDATION_CONCURRENCY = 100;
const DOCUMENT_VALIDATION_TIMEOUT = 3e4;
const REFERENCE_INTEGRITY_BATCH_SIZE = 100;
const {
  clientConfig,
  workDir,
  workspace: workspaceName,
  configPath,
  dataset,
  projectId,
  level,
  maxCustomValidationConcurrency
} = worker_threads.workerData;
if (worker_threads.isMainThread || !worker_threads.parentPort) {
  throw new Error("This module must be run as a worker thread");
}
const levelValues = {
  error: 0,
  warning: 1,
  info: 2
};
const report = workerChannels.createReporter(worker_threads.parentPort);
const getReferenceIds = value => {
  const ids = /* @__PURE__ */new Set();
  function traverse(node) {
    if (types.isReference(node)) {
      ids.add(node._ref);
      return;
    }
    if (typeof node === "object" && node) {
      for (const item of Object.values(node)) {
        traverse(item);
      }
    }
  }
  traverse(value);
  return ids;
};
const idRegex = /^[^-][A-Z0-9._-]*$/i;
const isValidId = id => typeof id === "string" && idRegex.test(id);
async function* readerGenerator(reader) {
  while (true) {
    const {
      value,
      done
    } = await reader.read();
    if (value) yield value;
    if (done) return;
  }
}
validateDocuments();
async function loadWorkspace() {
  const workspaces = await getStudioConfig.getStudioConfig({
    basePath: workDir,
    configPath
  });
  if (!workspaces.length) {
    throw new Error("Configuration did not return any workspaces.");
  }
  let _workspace;
  if (workspaceName) {
    _workspace = workspaces.find(w => w.name === workspaceName);
    if (!_workspace) {
      throw new Error("Could not find any workspaces with name `".concat(workspaceName, "`"));
    }
  } else {
    if (workspaces.length !== 1) {
      throw new Error("Multiple workspaces found. Please specify which workspace to use with '--workspace'.");
    }
    _workspace = workspaces[0];
  }
  const workspace = _workspace;
  const client$1 = client.createClient({
    ...clientConfig,
    dataset: dataset || workspace.dataset,
    projectId: projectId || workspace.projectId,
    // we set this explictly to true because the default client configuration
    // from the CLI comes configured with `useProjectHostname: false` when
    // `requireProject` is set to false
    useProjectHostname: true,
    // we set this explictly to true because we pass in a token via the
    // `clientConfiguration` object and also mock a browser environment in
    // this worker which triggers the browser warning
    ignoreBrowserTokenWarning: true,
    requestTagPrefix: "sanity.cli.validate"
  }).config({
    apiVersion: "v2021-03-25"
  });
  let studioHost;
  try {
    const project = await client$1.projects.getById(projectId || workspace.projectId);
    studioHost = project.metadata.externalStudioHost || project.studioHost;
  } catch {
    studioHost = null;
  }
  report.event.loadedWorkspace({
    projectId: workspace.projectId,
    dataset: workspace.dataset,
    name: workspace.name,
    studioHost,
    basePath: workspace.basePath
  });
  return {
    workspace,
    client: client$1
  };
}
async function downloadDocuments(client) {
  var _a;
  const exportUrl = new URL(client.getUrl("/data/export/".concat(client.config().dataset), false));
  const documentCount = await client.fetch("length(*)");
  report.event.loadedDocumentCount({
    documentCount
  });
  const {
    token
  } = client.config();
  const response = await fetch(exportUrl, {
    headers: new Headers({
      ...(token && {
        Authorization: "Bearer ".concat(token)
      })
    })
  });
  const reader = (_a = response.body) == null ? void 0 : _a.getReader();
  if (!reader) throw new Error("Could not get reader from response body.");
  const lines = readline__default.default.createInterface({
    input: stream.Readable.from(readerGenerator(reader))
  });
  let downloadedCount = 0;
  const referencedIds = /* @__PURE__ */new Set();
  const documentIds = /* @__PURE__ */new Set();
  const slugDate = ( /* @__PURE__ */new Date()).toISOString().replace(/[^a-z0-9]/gi, "-").toLowerCase();
  const tempOutputFile = path__default.default.join(os__default.default.tmpdir(), "sanity-validate-".concat(slugDate, ".ndjson"));
  const outputStream = fs__default.default.createWriteStream(tempOutputFile);
  for await (const line of lines) {
    const document = JSON.parse(line);
    documentIds.add(document._id);
    for (const referenceId of getReferenceIds(document)) {
      referencedIds.add(referenceId);
    }
    outputStream.write("".concat(line, "\n"));
    downloadedCount++;
    report.stream.exportProgress.emit({
      downloadedCount,
      documentCount
    });
  }
  await new Promise((resolve, reject) => outputStream.close(err => err ? reject(err) : resolve()));
  async function* getDocuments() {
    const rl = readline__default.default.createInterface({
      input: fs__default.default.createReadStream(tempOutputFile)
    });
    for await (const line of rl) {
      if (line) {
        yield JSON.parse(line);
      }
    }
    rl.close();
  }
  report.stream.exportProgress.end();
  return {
    getDocuments,
    documentIds,
    referencedIds,
    tempOutputFile
  };
}
async function checkReferenceExistence(_ref) {
  let {
    client,
    documentIds,
    referencedIds: _referencedIds
  } = _ref;
  const existingIds = new Set(documentIds);
  const idsToCheck = Array.from(_referencedIds).filter(id => !existingIds.has(id) && isValidId(id)).sort();
  const batches = idsToCheck.reduce((acc, next, index) => {
    const batchIndex = Math.floor(index / REFERENCE_INTEGRITY_BATCH_SIZE);
    const batch = acc[batchIndex];
    batch.push(next);
    return acc;
  }, Array.from({
    length: Math.ceil(idsToCheck.length / REFERENCE_INTEGRITY_BATCH_SIZE)
  }).fill([]));
  for (const batch of batches) {
    const {
      omitted
    } = await client.request({
      uri: client.getDataUrl("doc", batch.join(",")),
      json: true,
      query: {
        excludeContent: "true"
      },
      tag: "documents-availability"
    });
    const omittedIds = omitted.reduce((acc, next) => {
      acc[next.id] = next.reason;
      return acc;
    }, {});
    for (const id of batch) {
      if (omittedIds[id] !== "existence") {
        existingIds.add(id);
      }
    }
  }
  report.event.loadedReferenceIntegrity();
  return {
    existingIds
  };
}
async function validateDocuments() {
  const {
    default: pMap
  } = await import('p-map');
  const cleanup = mockBrowserEnvironment.mockBrowserEnvironment(workDir);
  let tempFile;
  try {
    const {
      client,
      workspace
    } = await loadWorkspace();
    const {
      getDocuments,
      documentIds,
      referencedIds,
      tempOutputFile
    } = await downloadDocuments(client);
    const {
      existingIds
    } = await checkReferenceExistence({
      client,
      referencedIds,
      documentIds
    });
    tempFile = tempOutputFile;
    const getClient = options => client.withConfig(options);
    const getDocumentExists = _ref2 => {
      let {
        id
      } = _ref2;
      return Promise.resolve(existingIds.has(id));
    };
    const getLevel = markers => {
      let foundWarning = false;
      for (const marker of markers) {
        if (marker.level === "error") return "error";
        if (marker.level === "warning") foundWarning = true;
      }
      if (foundWarning) return "warning";
      return "info";
    };
    let validatedCount = 0;
    const validate = async document => {
      let markers;
      try {
        const timeout = Symbol("timeout");
        const result = await Promise.race([sanity.validateDocument({
          document,
          workspace,
          getClient,
          getDocumentExists,
          environment: "cli",
          maxCustomValidationConcurrency
        }), new Promise(resolve => setTimeout(() => resolve(timeout), DOCUMENT_VALIDATION_TIMEOUT))]);
        if (result === timeout) {
          throw new Error("Document '".concat(document._id, "' failed to validate within ").concat(DOCUMENT_VALIDATION_TIMEOUT, "ms."));
        }
        markers = result.map(_ref3 => {
          let {
            item,
            ...marker
          } = _ref3;
          return marker;
        }).filter(marker => {
          var _a;
          const markerValue = levelValues[marker.level];
          const flagLevelValue = (_a = levelValues[level]) != null ? _a : levelValues.info;
          return markerValue <= flagLevelValue;
        });
      } catch (err) {
        const errorMessage = sanity.isRecord(err) && typeof err.message === "string" ? err.message : "Unknown error";
        const message = "Exception occurred while validating value: ".concat(errorMessage);
        markers = [{
          message,
          level: "error",
          path: []
        }];
      }
      validatedCount++;
      report.stream.validation.emit({
        documentId: document._id,
        documentType: document._type,
        revision: document.rev,
        markers,
        validatedCount,
        level: getLevel(markers)
      });
    };
    await pMap(getDocuments(), validate, {
      concurrency: MAX_VALIDATION_CONCURRENCY
    });
    report.stream.validation.end();
  } finally {
    cleanup();
    if (tempFile && fs__default.default.existsSync(tempFile)) {
      await fs__default.default.promises.rm(tempFile);
    }
  }
}
//# sourceMappingURL=validateDocuments.js.map
